# -*- coding: utf-8 -*-
"""AskFromWeb: A Web-Based Q&A System Using URL-Driven Content.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHatznRTFoZ8yLCNyWNJEPA70nrF0GdC

# ***Connecting With Cuda First And Chech It Connected Or Not***
"""

! nvidia-smi

"""# ***Install All The Dependency***"""

!pip install streamlit pyngrok requests beautifulsoup4 faiss-cpu sentence-transformers transformers

"""# ***Use BeautifulSoup To Extract The Text***"""

import requests
from bs4 import BeautifulSoup

def extract_text_from_urls(urls):
    text_data = []
    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        # Remove script/style
        for tag in soup(["script", "style"]):
            tag.decompose()
        # Extract visible text
        text = soup.get_text(separator=' ', strip=True)
        text_data.append(text)
    return text_data

"""# ***Extract WikiPidea Deep_Learning Article Using Beautiful Soup***"""

urls = ["https://en.wikipedia.org/wiki/Deep_learning"]

web_text = extract_text_from_urls(urls)

# Print first 1000 characters from the first webpage
print(web_text[0][:1000])

"""# ***Create The Embedding using sentence-transformers + Vector Search using FAISS***"""

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

"""# ***Load The ALL-MiniLM-L6-v2 Model Using Sentence Transformers We Can Use Hugging-Face-Hub Model Also***"""

# Load the model Using Sentence Transformers
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

"""# ***Take The Example Text Chunk For Embedding Purpose***"""

text_chunks = [
    "Deep learning is a subset of machine learning that uses neural networks.",
    "Machine learning involves training models on data.",
    "Natural language processing helps machines understand human language.",
    "Neural networks are used in both deep learning and image recognition."
]

"""# ***Create The Embedding***"""

embeddings = embed_model.encode(text_chunks)

# Print embeddings (first one)
print("First Embedding Vector (chunk 1):")
print(embeddings[0])

# Check shape
print("\nShape of All Embeddings:", embeddings.shape)

"""# ***Seeing All The Embeddings***"""

for i, emb in enumerate(embeddings):
    print(f"\nüîπ Embedding {i+1} for chunk: {text_chunks[i]}")
    print(emb)

"""# ***Create The Chunk Data Function***"""

def split_into_chunks(text_list, chunk_size=200):
    chunks = []
    for text in text_list:
        words = text.split()
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i+chunk_size])
            chunks.append(chunk)
    return chunks

"""# ***Create FAISS index function***"""

def create_faiss_index(chunks):
    embeddings = embed_model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return index, embeddings

"""# ***Create The Gemini Pro API Key With os Environment + Add Langmith Also For Tracing Purposes***"""

import os
from google.colab import userdata

! pip install google-generativeai

import google.generativeai as genai
genai.configure(api_key="AIzaSyDGqVBzzrF3lFh3GK8Gi4ZdtC3h206Bq98")
os.environ["LANGCHAIN_PROJECT"] = "Web-Based-Tracing-Using-Langsmith"
os.environ["LANGCHAIN_TRACING_V2"] = "true"

"""# ***Create The Gemini Pro LLM For Getting Answer Related to Our Context***"""

llm = genai.GenerativeModel(model_name = "models/gemini-1.5-pro")
def answer_question(question, embed_model, index, chunks):
    question_embedding = embed_model.encode([question])
    D, I = index.search(np.array(question_embedding), k=5)
    retrieved_chunks = [chunks[i] for i in I[0]]

    # Combine into one context
    context = "\n".join(retrieved_chunks)

    # Generate answer using Gemini
    prompt = f"""You are a helpful AI assistant. Answer the following question strictly using the provided content.

     Context:
    {context}

    Question: {question}

    Answer:"""

    response = llm.generate_content(prompt)
    return response.text, context

urls = [
    "https://www.geeksforgeeks.org/machine-learning/",
    "https://www.ibm.com/topics/deep-learning"
]

"""# ***Function For Getting Urls***"""

def get_urls_from_user():
    print("üîó Enter URLs one by one. Type 'done' when finished.\n")
    urls = []
    while True:
        url = input("Enter URL: ")
        if url.lower() == 'done':
            break
        if url.startswith("http://") or url.startswith("https://"):
            urls.append(url)
        else:
            print("‚ö†Ô∏è Please enter a valid URL (starting with http or https).")
    return urls

urls = get_urls_from_user()

raw_text = extract_text_from_urls(urls)
chunks = split_into_chunks(raw_text, chunk_size=150)
faiss_index, _ = create_faiss_index(chunks)

"""# ***Test Our Model Using Gemin-Pro-1.5-Vision***"""

q = input("Ask The Question Realed to Provided Url")
answer, context = answer_question(q, embed_model, faiss_index, chunks)
print("\nüß† Answer:", answer.strip())
print("\nüìå Context Preview:\n", context[:400], "\n" + "-"*80)

"""# ***Install Gradio***"""

! pip install gradio

"""# ***Live Running My Project Using Gradio***"""

import gradio as gr
def generate_answer_from_chunk(question, top_chunk, chunk_list, idx, num_neighbors=5):

    context_start = max(0, idx - num_neighbors)
    context_end = min(len(chunk_list), idx + num_neighbors + 1)
    full_context = " ".join(chunk_list[context_start:context_end])

    # Return the generated answer with a more comprehensive context
    return f"Based on the content, here's what was found: {top_chunk[:800]}..."


def run_qa(url_input, question):
    urls = [u.strip() for u in url_input.split(",") if u.strip().startswith("http")]
    if not urls or not question.strip():
        return "‚ùó Please enter valid URLs and a question.", ""

    texts = [fetch_text(u) for u in urls]
    chunks = chunk_text(texts)
    index, chunk_list = build_index(chunks)

    # Search relevant chunk
    q_embed = embed_model.encode([question], normalize_embeddings=True)
    _, I = index.search(np.array(q_embed), k=3)
    top_chunk = chunk_list[I[0][0]]

    # Get more context around the selected chunk (using neighboring chunks)
    answer = generate_answer_from_chunk(question, top_chunk, chunk_list, I[0][0])

    # Use the surrounding context as preview
    context = " ".join(chunk_list[max(0, I[0][0] - 2):min(len(chunk_list), I[0][0] + 3)])[:600] + "..."

    return answer, context


gr.Interface(
    fn=run_qa,
    inputs=[
        gr.Textbox(label="üåê Enter URLs", placeholder="https://example.com, https://another.com"),
        gr.Textbox(label="‚ùì Your Question", placeholder="e.g. What is AI?")
    ],
    outputs=[
        gr.Textbox(label="üß† Smart Answer from Page"),
        #gr.Textbox(label="üìå Full Context Preview")

    ],
    title="‚ö°Web Based Q&A Bot",
    description="Ask questions from websites URLs instantly using this Web Based Chatbot.",


).launch(share=True)